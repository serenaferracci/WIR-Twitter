\documentclass[journal,11pt]{vgtc}  

\usepackage{mathptmx}
\usepackage{graphicx}
\usepackage{times}

\usepackage[bookmarks,backref=true,linkcolor=black]{hyperref} %,colorlinks
\hypersetup{
  pdfauthor = {},
  pdftitle = {},
  pdfsubject = {},
  pdfkeywords = {},
  colorlinks=true,
  linkcolor= black,
  citecolor= black,
  pageanchor=true,
  urlcolor = black,
  plainpages = false,
  linktocpage
}

\onlineid{0}


\title{Twitter Trending Topic Classification}
\author{MES: Maria Ludovica Costagliola - Emanuele De Santis - Serena Ferracci}

%% Abstract section.
\abstract{
  \normalsize
  The Web Information Retrieval course allowed us to develope the described project.
  It is about classification of topics that are trending of twitter in a given moment. We have dealt 
  with (i) text based classification using single tweets (ii) network based classification exploiting the 
  graph structure of Twitter. 

  The used dataset is composed by tweets divided by trending topics. 
  We classify the trending topics into 8 categories: \textit{Event, Health, Movie, Music, Politics, Science, Society} and \textit{Sport}.

  The first approach gives good result in terms of precision and recall. Instead, the second approach 
  does not give conclusive results because it require too many resources to be implemented on a single machine.

} 


\begin{document}


\maketitle

\section{Introduction}
We chose this project because Twitter is one of the most popular social network. 
It is used every day by millions of users expressing their opinions about several fields.
When an user looks for something, the first thing Twitter displays to him is the list of 
trending topics of the moment. Often the user can not know what the topic is about, so it as to manually 
search for tweets belonging to that trending topic to better understand it.

It is interesting for the user to have a way to know what the Trending Topic is about
without further searches.
Our work tries to replicate the results obtained by \cite{lee_palsetia_narayanan_patwary_agrawal_choudhary_2011}.  
The general categories used in this project are 8, named: \textit{Event, Health, Movie, Music, Politics, Science, Society} and \textit{Sport}.


\section{Related Work}
The paper we refer to is the work done by Lee et al. for tweets classification \cite{lee_palsetia_narayanan_patwary_agrawal_choudhary_2011}. 
They have used 18 general categories to classify each trending topic.
They address the problem following two different approaches. The first one is a supervised learning technique, named
Multinomial Naive Bayes. Instead, the second one is network based and it uses Personalized PageRank to compute the 
top-k influencers and then it computes the intersection between the influencers to classify the new trending topic.

All trending topics they used were downloaded from \textit{what the trend}.

\section{Dataset}
Since it was not possible to get a suitable dataset for our purpose, we needed to build it manually
using Twitter APIs \cite{twitter} through Tweepy \cite{api}.

In \texttt{tweets\_retrieve.py} we
first retrieved trending topics from USA using USA WOEID (Where On Earth IDentifier, that is 23424977 for USA). 
Then, for each trending topic, we queried Twitter to get all tweets belonging to the given topic.
We pay attention to retrieve the entire text of the tweet; in case of retweeted statuses we consider only the 
retweeted text. 
Twitter imposes some limitation on GET requests, in particular we could retrieve 180 tweets every 15 minutes.
So, we have to handle exceptions fired due to the reached limit.
In order to submit a request we have to set up Tweepy authorization handler with our
consumer key and our access token, we take from our twitter developer account.
As result, we get a file for each tweet where its name is the ID of the author and the body is the text of his tweet.
This file is placed in a folder whose name is composed by the name of the trending topic.
At the end, we collect about 20.000 tweets belonging to 139 trending topics.

After this first phase, we had to retrieve the link structure for the users that wrote the tweets collected 
in the previous phase. In order to do so, we developed two python files, named \texttt{followers\_retrieve} and 
\texttt{friends\_retrieve}. 
We used Tweepy to retrieve all followers and friends for each author ID being aware of all exceptions.
Since the limitation, in this case, is 15 queries every 15 minutes, we had to find a way to speed up the process.
To do so we decided to parallelize the computation using more than one key (precisely 6 keys), contrarily with respect to the previous 
retrieving phase, and working in data separation. We associated each key to a subset of IDs to be processed, this
mapping was done using a simple hash function. 
We saved the list of followers and the list of friends for an user $x$ in the files named \texttt{x - followers.txt}
and \texttt{x - friends.txt}. These two files are saved in the same folder that contains the tweet tweeted by $x$.

We have to deal with several problems to retrieve the dataset:
\begin{itemize}
  \item some users changed their privacy settings between the first and the second phase
  of our retrieving. For this reason we couldn't get neither their followers nor their friends.
  \item a subset of users have a plethora of followers, of the order of millions of users. 
  It took a lot of time to complete the retrieving of their followers, sometimes almost an 
  entire day at full capacity.
  It also happened that the execution stopped due to connection problems or other unpredictable events, 
  so it was not possible to retrieve the entire list of IDs.
  
\end{itemize}


\bibliographystyle{abbrv}
%%use following if all content of bibtex file should be shown
%\nocite{*}
\bibliography{template}
\end{document}
